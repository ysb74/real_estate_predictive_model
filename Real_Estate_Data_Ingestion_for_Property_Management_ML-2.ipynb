{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "\n",
        "# --- Configuration for Real-World APIs/Systems ---\n",
        "# Replace with your actual API keys and endpoints in a real application\n",
        "ZILLOW_API_KEY = \"YOUR_ZILLOW_API_KEY\"\n",
        "REALTOR_API_KEY = \"YOUR_REALTOR_API_KEY\"\n",
        "# Example API Endpoints (these are illustrative, not actual working endpoints)\n",
        "ZILLOW_PROPERTY_API_ENDPOINT = \"https://api.zillow.com/v1/property_details\"\n",
        "REALTOR_LISTINGS_API_ENDPOINT = \"https://api.realtor.com/v1/listings\"\n",
        "# Hypothetical CRM API endpoint (e.g., for Salesforce, HubSpot, or a custom CRM)\n",
        "CRM_API_ENDPOINT = \"https://your-crm-instance.com/api/v2/tenants\"\n",
        "CRM_AUTH_TOKEN = \"YOUR_CRM_AUTH_TOKEN\"\n",
        "\n",
        "# --- 1. Data Ingestion from CRM Systems (Conceptual Live Access) ---\n",
        "# In a real scenario, you would use the CRM's official API or SDK.\n",
        "# Popular real estate CRMs include Salesforce, HubSpot, Follow Up Boss, Propertybase, etc.\n",
        "# Each would have its own authentication and data retrieval methods.\n",
        "\n",
        "def load_crm_data_live():\n",
        "    \"\"\"\n",
        "    Simulates loading tenant and property data directly from a CRM system's API.\n",
        "    This function outlines the conceptual steps for API interaction.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing CRM data (simulated for this example).\n",
        "    \"\"\"\n",
        "    print(f\"--- Attempting to load CRM data from {CRM_API_ENDPOINT} ---\")\n",
        "    try:\n",
        "        # In a real scenario, you'd make an authenticated API call:\n",
        "        # headers = {\n",
        "        #     \"Authorization\": f\"Bearer {CRM_AUTH_TOKEN}\",\n",
        "        #     \"Content-Type\": \"application/json\"\n",
        "        # }\n",
        "        # response = requests.get(CRM_API_ENDPOINT, headers=headers, params={\"status\": \"active_tenants\"})\n",
        "        # response.raise_for_status() # Raise an HTTPError for bad responses\n",
        "        # crm_raw_data = response.json()\n",
        "\n",
        "        # --- Simulated CRM API Response for Demonstration ---\n",
        "        crm_raw_data = {\n",
        "            \"tenants\": [\n",
        "                {'tenant_id': 101, 'lease_start': '2023-01-15', 'lease_end': '2024-01-15', 'rent': 1500, 'payments_late_6m': 0, 'maint_req_12m': 1, 'feedback_score': 4.5, 'prop_id': 'P001', 'status': 'Active'},\n",
        "                {'tenant_id': 102, 'lease_start': '2022-07-01', 'lease_end': '2024-07-01', 'rent': 2000, 'payments_late_6m': 1, 'maint_req_12m': 2, 'feedback_score': 3.8, 'prop_id': 'P002', 'status': 'Active'},\n",
        "                {'tenant_id': 103, 'lease_start': '2023-03-20', 'lease_end': '2024-03-20', 'rent': 1200, 'payments_late_6m': 3, 'maint_req_12m': 4, 'feedback_score': 2.5, 'prop_id': 'P001', 'status': 'Active'},\n",
        "                {'tenant_id': 104, 'lease_start': '2024-01-10', 'lease_end': '2025-01-10', 'rent': 1800, 'payments_late_6m': 0, 'maint_req_12m': 0, 'feedback_score': 4.9, 'prop_id': 'P003', 'status': 'Active'},\n",
        "                {'tenant_id': 105, 'lease_start': '2022-11-05', 'lease_end': '2024-11-05', 'rent': 2500, 'payments_late_6m': 2, 'maint_req_12m': 3, 'feedback_score': 3.1, 'prop_id': 'P002', 'status': 'Active'},\n",
        "            ]\n",
        "        }\n",
        "        # --- End Simulated CRM API Response ---\n",
        "\n",
        "        df_crm = pd.DataFrame(crm_raw_data.get(\"tenants\", []))\n",
        "        # Rename columns to match the expected schema for aggregation\n",
        "        df_crm.rename(columns={\n",
        "            'lease_start': 'lease_start_date',\n",
        "            'lease_end': 'lease_end_date',\n",
        "            'rent': 'monthly_rent',\n",
        "            'payments_late_6m': 'payment_delays_last_6_months',\n",
        "            'maint_req_12m': 'maintenance_requests_last_year',\n",
        "            'prop_id': 'property_id'\n",
        "        }, inplace=True)\n",
        "        print(\"Successfully loaded CRM data via simulated API.\")\n",
        "        return df_crm\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error connecting to CRM API: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred processing CRM data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# --- 2. Web Scraping (Conceptual for Publicly Available Listing Info) ---\n",
        "# This function demonstrates how you would use requests and BeautifulSoup to scrape\n",
        "# publicly available information from real estate websites.\n",
        "# Always adhere to robots.txt and terms of service. Avoid aggressive scraping.\n",
        "\n",
        "def scrape_public_listings(search_url='https://www.example-real-estate-listings.com/search?location=Anytown', num_pages=1):\n",
        "    \"\"\"\n",
        "    Conceptual function to scrape publicly available property listing data.\n",
        "    This example targets a hypothetical real estate listing site.\n",
        "\n",
        "    Args:\n",
        "        search_url (str): The base URL for the property search results.\n",
        "        num_pages (int): Number of pages to scrape.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each representing a scraped listing.\n",
        "    \"\"\"\n",
        "    scraped_data = []\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    print(f\"--- Attempting to scrape public listings from {search_url} ---\")\n",
        "    for page in range(1, num_pages + 1):\n",
        "        # In a real scenario, adjust page_url for actual website's pagination\n",
        "        page_url = f\"{search_url}&page={page}\" if num_pages > 1 else search_url\n",
        "        try:\n",
        "            response = requests.get(page_url, headers=headers)\n",
        "            response.raise_for_status() # Raise an HTTPError for bad responses\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # --- Placeholder for actual real estate parsing logic ---\n",
        "            # You would inspect the HTML of Zillow/Realtor.com (public sections)\n",
        "            # or other listing sites to find specific CSS selectors for:\n",
        "            # - Property Address\n",
        "            # - Price\n",
        "            # - Number of Beds/Baths\n",
        "            # - Square Footage\n",
        "            # - Listing URL\n",
        "            # - Listing Agent/Broker (if available publicly)\n",
        "\n",
        "            # Example: Simulate finding property cards and extracting basic info\n",
        "            simulated_listing_cards = [\n",
        "                {'address': '123 Main St, Anytown', 'price_str': '$355,000', 'beds_str': '3 beds', 'baths_str': '2 baths', 'sqft_str': '1,500 sqft'},\n",
        "                {'address': '456 Oak Ave, Anytown', 'price_str': '$510,000', 'beds_str': '4 beds', 'baths_str': '3 baths', 'sqft_str': '2,250 sqft'},\n",
        "            ]\n",
        "\n",
        "            for listing in simulated_listing_cards:\n",
        "                scraped_data.append({\n",
        "                    'scraped_address': listing['address'],\n",
        "                    'scraped_price_raw': listing['price_str'],\n",
        "                    'scraped_beds_raw': listing['beds_str'],\n",
        "                    'scraped_baths_raw': listing['baths_str'],\n",
        "                    'scraped_sqft_raw': listing['sqft_str'],\n",
        "                    'scraped_source_url': page_url # Link back to the source page\n",
        "                })\n",
        "            # --- End Placeholder ---\n",
        "\n",
        "            print(f\"Scraped page {page} successfully (conceptual).\")\n",
        "            time.sleep(random.uniform(2, 5)) # Be polite and add a delay\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error scraping page {page_url}: {e}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during scraping: {e}\")\n",
        "            break\n",
        "    return scraped_data\n",
        "\n",
        "# --- 3. Real Estate APIs (Conceptual Official Access: Zillow/Realtor.com) ---\n",
        "# This demonstrates making API calls to Zillow/Realtor.com.\n",
        "# Requires valid API keys and adherence to their usage policies.\n",
        "\n",
        "def get_property_data_from_official_api(api_name='Zillow', location='Anytown', property_type='house'):\n",
        "    \"\"\"\n",
        "    Simulates fetching property data from official Zillow or Realtor.com APIs.\n",
        "\n",
        "    Args:\n",
        "        api_name (str): 'Zillow' or 'Realtor'.\n",
        "        location (str): Search location.\n",
        "        property_type (str): Type of property (e.g., 'house', 'condo').\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each representing a property from the API.\n",
        "    \"\"\"\n",
        "    api_key = ZILLOW_API_KEY if api_name == 'Zillow' else REALTOR_API_KEY\n",
        "    api_endpoint = ZILLOW_PROPERTY_API_ENDPOINT if api_name == 'Zillow' else REALTOR_LISTINGS_API_ENDPOINT\n",
        "\n",
        "    print(f\"--- Attempting to fetch data from {api_name} API for {location} ---\")\n",
        "    if api_key == \"YOUR_ZILLOW_API_KEY\" or api_key == \"YOUR_REALTOR_API_KEY\":\n",
        "        print(f\"Warning: Using placeholder API key for {api_name}. Real API calls will fail without a genuine key.\")\n",
        "\n",
        "    try:\n",
        "        # In a real scenario, you'd construct the actual API request based on their docs:\n",
        "        # headers = {\"Accept\": \"application/json\"}\n",
        "        # params = {\"location\": location, \"property_type\": property_type, \"api_key\": api_key}\n",
        "        # response = requests.get(api_endpoint, headers=headers, params=params)\n",
        "        # response.raise_for_status()\n",
        "        # api_raw_data = response.json()\n",
        "\n",
        "        # --- Simulated API Response for Demonstration ---\n",
        "        if api_name == 'Zillow':\n",
        "            api_raw_data = {\n",
        "                \"properties\": [\n",
        "                    {\"zpid\": \"P001\", \"address\": {\"street\": \"123 Main St\", \"city\": \"Anytown\"}, \"bedrooms\": 3, \"bathrooms\": 2, \"squareFootage\": 1500, \"zestimate\": 350500, \"lastSoldDate\": \"2020-05-10\"},\n",
        "                    {\"zpid\": \"P002\", \"address\": {\"street\": \"456 Oak Ave\", \"city\": \"Anytown\"}, \"bedrooms\": 4, \"bathrooms\": 3, \"squareFootage\": 2200, \"zestimate\": 505000, \"lastSoldDate\": \"2021-01-20\"},\n",
        "                    {\"zpid\": \"P003\", \"address\": {\"street\": \"789 Pine Ln\", \"city\": \"Anytown\"}, \"bedrooms\": 2, \"bathrooms\": 1, \"squareFootage\": 900, \"zestimate\": 200100, \"lastSoldDate\": \"2019-11-15\"}\n",
        "                ],\n",
        "                \"status\": \"success\"\n",
        "            }\n",
        "        elif api_name == 'Realtor':\n",
        "            api_raw_data = {\n",
        "                \"listings\": [\n",
        "                    {\"listing_id\": \"L001\", \"location\": {\"address\": \"123 Main St, Anytown\"}, \"beds\": 3, \"baths\": 2, \"sq_ft\": 1500, \"list_price\": 350000, \"sale_date\": \"2020-05-10\"},\n",
        "                    {\"listing_id\": \"L002\", \"location\": {\"address\": \"456 Oak Ave, Anytown\"}, \"beds\": 4, \"baths\": 3, \"sq_ft\": 2200, \"list_price\": 500000, \"sale_date\": \"2021-01-20\"},\n",
        "                    {\"listing_id\": \"L003\", \"location\": {\"address\": \"789 Pine Ln, Anytown\"}, \"beds\": 2, \"baths\": 1, \"sq_ft\": 900, \"list_price\": 200000, \"sale_date\": \"2019-11-15\"}\n",
        "                ],\n",
        "                \"status\": \"success\"\n",
        "            }\n",
        "        else:\n",
        "            api_raw_data = {\"status\": \"error\", \"message\": \"Invalid API name\"}\n",
        "        # --- End Simulated API Response ---\n",
        "\n",
        "        if api_raw_data.get(\"status\") == \"success\":\n",
        "            if api_name == 'Zillow':\n",
        "                # Normalize Zillow data\n",
        "                normalized_data = []\n",
        "                for prop in api_raw_data.get(\"properties\", []):\n",
        "                    normalized_data.append({\n",
        "                        'api_source': 'Zillow',\n",
        "                        'property_api_id': prop.get('zpid'),\n",
        "                        'address': f\"{prop.get('address', {}).get('street')}, {prop.get('address', {}).get('city')}\",\n",
        "                        'beds': prop.get('bedrooms'),\n",
        "                        'baths': prop.get('bathrooms'),\n",
        "                        'sqft': prop.get('squareFootage'),\n",
        "                        'price': prop.get('zestimate'),\n",
        "                        'last_sold_date': prop.get('lastSoldDate')\n",
        "                    })\n",
        "                print(f\"Successfully fetched and normalized data from {api_name} API.\")\n",
        "                return normalized_data\n",
        "            elif api_name == 'Realtor':\n",
        "                # Normalize Realtor.com data\n",
        "                normalized_data = []\n",
        "                for listing in api_raw_data.get(\"listings\", []):\n",
        "                    normalized_data.append({\n",
        "                        'api_source': 'Realtor',\n",
        "                        'property_api_id': listing.get('listing_id'),\n",
        "                        'address': listing.get('location', {}).get('address'),\n",
        "                        'beds': listing.get('beds'),\n",
        "                        'baths': listing.get('baths'),\n",
        "                        'sqft': listing.get('sq_ft'),\n",
        "                        'price': listing.get('list_price'),\n",
        "                        'last_sold_date': listing.get('sale_date')\n",
        "                    })\n",
        "                print(f\"Successfully fetched and normalized data from {api_name} API.\")\n",
        "                return normalized_data\n",
        "        else:\n",
        "            print(f\"{api_name} API call failed: {api_raw_data.get('message', 'Unknown error')}\")\n",
        "            return []\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during {api_name} API call: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred processing {api_name} API data: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- 4. Data Aggregation and Integration ---\n",
        "def aggregate_real_estate_data(crm_df, scraped_data_list, api_data_list):\n",
        "\n",
        "    print(\"\\n--- Aggregating Data from Multiple Sources ---\")\n",
        "\n",
        "    # Convert scraped and API data to DataFrames\n",
        "    df_scraped = pd.DataFrame(scraped_data_list)\n",
        "    df_api = pd.DataFrame(api_data_list)\n",
        "\n",
        "    unified_df = crm_df.copy()\n",
        "    print(f\"Starting with CRM data. Shape: {unified_df.shape}\")\n",
        "\n",
        "    # Merge with API data (using property_id from CRM and property_api_id from API)\n",
        "    if not df_api.empty:\n",
        "        # Pre-process API data to ensure 'property_id' is consistent for merging\n",
        "        # This is a simplification; real matching might need fuzzy string matching on address\n",
        "        df_api['property_id'] = df_api['property_api_id'] # Assume direct mapping for demo\n",
        "\n",
        "        # Select relevant API columns and drop duplicates based on property_id\n",
        "        api_cols_to_merge = ['property_id', 'beds', 'baths', 'sqft', 'price', 'last_sold_date']\n",
        "        df_api_filtered = df_api[api_cols_to_merge].drop_duplicates(subset=['property_id'])\n",
        "\n",
        "        # Merge API data into unified_df\n",
        "        unified_df = pd.merge(unified_df, df_api_filtered, on='property_id', how='left', suffixes=('_crm', '_api'))\n",
        "        print(f\"Merged with API data. New shape: {unified_df.shape}\")\n",
        "    else:\n",
        "        print(\"No API data to merge.\")\n",
        "\n",
        "    # Integrate scraped data (often harder to merge directly due to lack of common IDs)\n",
        "    # For demonstration, we'll add scraped data as new columns if addresses match conceptually.\n",
        "    # In a real scenario, this would involve robust address matching or Geo-coding.\n",
        "    if not df_scraped.empty:\n",
        "        print(f\"Scraped data available. Shape: {df_scraped.shape}. Manual or fuzzy matching may be needed.\")\n",
        "        # Example: If we could match by address, we'd do another merge here\n",
        "        # unified_df = pd.merge(unified_df, df_scraped, left_on='address_from_crm', right_on='scraped_address', how='left')\n",
        "        # For this demo, we'll just show the scraped data as a separate entity or append if schemas align.\n",
        "\n",
        "        # For simplicity, let's just add a placeholder column if scraped data exists\n",
        "        unified_df['has_scraped_listing'] = unified_df['property_id'].apply(\n",
        "            lambda pid: 1 if any(f\"P{i}\" in s['scraped_address'] for i, s in enumerate(scraped_data_list)) else 0\n",
        "        )\n",
        "        print(\"Added 'has_scraped_listing' indicator (conceptual).\")\n",
        "    else:\n",
        "        print(\"No scraped data to integrate.\")\n",
        "\n",
        "    # Post-aggregation cleaning/harmonization\n",
        "    # Resolve conflicting columns, e.g., 'beds_crm' vs 'beds_api'\n",
        "    for col in ['beds', 'baths', 'sqft', 'price']:\n",
        "        if f'{col}_api' in unified_df.columns and f'{col}_crm' in unified_df.columns:\n",
        "            # Prioritize API data if available, otherwise use CRM data\n",
        "            unified_df[col] = unified_df[f'{col}_api'].fillna(unified_df[f'{col}_crm'])\n",
        "            unified_df.drop(columns=[f'{col}_crm', f'{col}_api'], inplace=True)\n",
        "        elif f'{col}_api' in unified_df.columns:\n",
        "            unified_df.rename(columns={f'{col}_api': col}, inplace=True)\n",
        "        elif f'{col}_crm' in unified_df.columns:\n",
        "            unified_df.rename(columns={f'{col}_crm': col}, inplace=True)\n",
        "        # Ensure numerical type\n",
        "        if col in unified_df.columns:\n",
        "            unified_df[col] = pd.to_numeric(unified_df[col], errors='coerce')\n",
        "            unified_df[col].fillna(unified_df[col].median(), inplace=True) # Fill any NaNs after merge\n",
        "\n",
        "    # Handle remaining date columns from API if they exist and are not already in CRM\n",
        "    if 'last_sold_date' in unified_df.columns:\n",
        "        unified_df['last_sold_date'] = pd.to_datetime(unified_df['last_sold_date'], errors='coerce')\n",
        "        unified_df['days_since_last_sold'] = (pd.to_datetime('now') - unified_df['last_sold_date']).dt.days.fillna(0)\n",
        "\n",
        "    # Drop temporary/raw columns from API/scraped data if not needed\n",
        "    unified_df.drop(columns=['api_source', 'property_api_id'] if 'api_source' in unified_df.columns else [], inplace=True, errors='ignore')\n",
        "\n",
        "    print(f\"Final unified DataFrame shape: {unified_df.shape}\")\n",
        "    return unified_df\n",
        "\n",
        "# --- Main Execution Flow ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Ingest CRM Data (Conceptual Live Access)\n",
        "    crm_data = load_crm_data_live()\n",
        "    print(\"\\n--- CRM Data Loaded (Conceptual Live) ---\")\n",
        "    print(crm_data.head())\n",
        "\n",
        "    # 2. Ingest Web Scraped Data (Conceptual)\n",
        "    # This will scrape from a safe example site. For real estate, replace with actual URLs.\n",
        "    scraped_data = scrape_public_listings(num_pages=1) # Reduced pages for faster demo\n",
        "    print(\"\\n--- Scraped Data Loaded (Conceptual) ---\")\n",
        "    if scraped_data:\n",
        "        print(pd.DataFrame(scraped_data).head())\n",
        "    else:\n",
        "        print(\"No scraped data to display.\")\n",
        "\n",
        "    # 3. Ingest API Data (Conceptual Official Access: Zillow/Realtor.com)\n",
        "    zillow_api_data = get_property_data_from_official_api(api_name='Zillow', location='Seattle, WA')\n",
        "    realtor_api_data = get_property_data_from_official_api(api_name='Realtor', location='Seattle, WA')\n",
        "    api_data_combined = zillow_api_data + realtor_api_data # Combine data from different APIs\n",
        "\n",
        "    print(\"\\n--- Combined API Data Loaded (Conceptual Official) ---\")\n",
        "    if api_data_combined:\n",
        "        print(pd.DataFrame(api_data_combined).head())\n",
        "    else:\n",
        "        print(\"No API data to display.\")\n",
        "\n",
        "    # 4. Aggregate all data sources\n",
        "    if not crm_data.empty:\n",
        "        final_unified_df = aggregate_real_estate_data(crm_data, scraped_data, api_data_combined)\n",
        "        print(\"\\n--- Final Unified Data Sample ---\")\n",
        "        print(final_unified_df.head())\n",
        "        print(f\"\\nFinal unified data columns: {final_unified_df.columns.tolist()}\")\n",
        "    else:\n",
        "        print(\"\\nCannot aggregate data: CRM data was not loaded.\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "BAd2Lxgy49dZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}