# -*- coding: utf-8 -*-
"""Real Estate Data Ingestion and Preprocessing Pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1arQx7hWcZJqWQuQrw8ezlsvWmJvY6e6o
"""

import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import json
import time
import random
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from scipy.stats import zscore
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score

# Import TensorFlow and Keras for Neural Networks
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
except ImportError:
    print("TensorFlow not installed. Neural Network models will be skipped.")
    tf = None # Set tf to None if import fails


# --- Configuration for Real-World APIs/Systems ---
# Replace with your actual API keys and endpoints in a real application
ZILLOW_API_KEY = "YOUR_ZILLOW_API_KEY"
REALTOR_API_KEY = "YOUR_REALTOR_API_KEY"
# Example API Endpoints (these are illustrative, not actual working endpoints)
ZILLOW_PROPERTY_API_ENDPOINT = "https://api.zillow.com/v1/property_details"
REALTOR_LISTINGS_API_ENDPOINT = "https://api.realtor.com/v1/listings"
# Hypothetical CRM API endpoint (e.g., for Salesforce, HubSpot, or a custom CRM)
CRM_API_ENDPOINT = "https://your-crm-instance.com/api/v2/tenants"
CRM_AUTH_TOKEN = "YOUR_CRM_AUTH_TOKEN"

# --- Data Ingestion Functions (from previous module) ---

def load_crm_data_live():
    """
    Simulates loading tenant and property data directly from a CRM system's API.
    This function outlines the conceptual steps for API interaction.

    Returns:
        pd.DataFrame: A DataFrame containing CRM data (simulated for this example).
    """
    print(f"--- Attempting to load CRM data from {CRM_API_ENDPOINT} ---")
    try:
        # In a real scenario, you'd make an authenticated API call:
        # headers = {
        #     "Authorization": f"Bearer {CRM_AUTH_TOKEN}",
        #     "Content-Type": "application/json"
        # }
        # response = requests.get(CRM_API_ENDPOINT, headers=headers, params={"status": "active_tenants"})
        # response.raise_for_status() # Raise an HTTPError for bad responses
        # crm_raw_data = response.json()

        # --- Simulated CRM API Response for Demonstration ---
        crm_raw_data = {
            "tenants": [
                {'tenant_id': 101, 'lease_start': '2023-01-15', 'lease_end': '2024-01-15', 'rent': 1500, 'payments_late_6m': 0, 'maint_req_12m': 1, 'feedback_score': 4.5, 'prop_id': 'P001', 'status': 'Active', 'lease_renewal_status': 'Renewed'},
                {'tenant_id': 102, 'lease_start': '2022-07-01', 'lease_end': '2024-07-01', 'rent': 2000, 'payments_late_6m': 1, 'maint_req_12m': 2, 'feedback_score': 3.8, 'prop_id': 'P002', 'status': 'Active', 'lease_renewal_status': 'Renewed'},
                {'tenant_id': 103, 'lease_start': '2023-03-20', 'lease_end': '2024-03-20', 'rent': 1200, 'payments_late_6m': 3, 'maint_req_12m': 4, 'feedback_score': 2.5, 'prop_id': 'P001', 'status': 'Active', 'lease_renewal_status': 'Churned'},
                {'tenant_id': 104, 'lease_start': '2024-01-10', 'lease_end': '2025-01-10', 'rent': 1800, 'payments_late_6m': 0, 'maint_req_12m': 0, 'feedback_score': 4.9, 'prop_id': 'P003', 'status': 'Active', 'lease_renewal_status': 'Renewed'},
                {'tenant_id': 105, 'lease_start': '2022-11-05', 'lease_end': '2024-11-05', 'rent': 2500, 'payments_late_6m': 2, 'maint_req_12m': 3, 'feedback_score': 3.1, 'prop_id': 'P002', 'status': 'Active', 'lease_renewal_status': 'Churned'},
            ]
        }
        # --- End Simulated CRM API Response ---

        df_crm = pd.DataFrame(crm_raw_data.get("tenants", []))
        # Rename columns to match the expected schema for aggregation
        df_crm.rename(columns={
            'lease_start': 'lease_start_date',
            'lease_end': 'lease_end_date',
            'rent': 'monthly_rent',
            'payments_late_6m': 'payment_delays_last_6_months',
            'maint_req_12m': 'maintenance_requests_last_year',
            'prop_id': 'property_id',
            'lease_renewal_status': 'lease_renewal' # Map 'Churned' to 0, 'Renewed' to 1
        }, inplace=True)
        # Convert lease_renewal to numerical
        df_crm['lease_renewal'] = df_crm['lease_renewal'].map({'Churned': 0, 'Renewed': 1})

        print("Successfully loaded CRM data via simulated API.")
        return df_crm
    except requests.exceptions.RequestException as e:
        print(f"Error connecting to CRM API: {e}")
        return pd.DataFrame()
    except Exception as e:
        print(f"An error occurred processing CRM data: {e}")
        return pd.DataFrame()

def scrape_public_listings(search_url='http://quotes.toscrape.com/', num_pages=1): # Changed default URL for safe demo
    """
    Conceptual function to scrape publicly available property listing data.
    This example targets a generic scraping site as a placeholder.

    Args:
        search_url (str): The base URL for the property search results.
        num_pages (int): Number of pages to scrape.

    Returns:
        list: A list of dictionaries, each representing a scraped listing.
    """
    scraped_data = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    print(f"--- Attempting to scrape public listings from {search_url} ---")
    for page in range(1, num_pages + 1):
        # In a real scenario, adjust page_url for actual website's pagination
        page_url = f"{search_url}/page/{page}/" if num_pages > 1 else search_url # Simple pagination for quotes.toscrape
        try:
            response = requests.get(page_url, headers=headers)
            response.raise_for_status() # Raise an HTTPError for bad responses
            soup = BeautifulSoup(response.text, 'html.parser')

            # --- Placeholder for actual real estate parsing logic ---
            # For quotes.toscrape.com, we extract quotes. For real estate, you'd extract property details.
            simulated_listing_cards = [
                {'address': f'123 Main St, Anytown - Scraped {page}', 'price_str': '$355,000', 'beds_str': '3 beds', 'baths_str': '2 baths', 'sqft_str': '1,500 sqft'},
                {'address': f'456 Oak Ave, Anytown - Scraped {page}', 'price_str': '$510,000', 'beds_str': '4 beds', 'baths_str': '3 baths', 'sqft_str': '2,250 sqft'},
            ]

            for listing in simulated_listing_cards:
                scraped_data.append({
                    'scraped_address': listing['address'],
                    'scraped_price_raw': listing['price_str'],
                    'scraped_beds_raw': listing['beds_str'],
                    'scraped_baths_raw': listing['baths_str'],
                    'scraped_sqft_raw': listing['sqft_str'],
                    'scraped_source_url': page_url # Link back to the source page
                })
            # --- End Placeholder ---

            print(f"Scraped page {page} successfully (conceptual).")
            time.sleep(random.uniform(1, 3)) # Be polite and add a delay
        except requests.exceptions.RequestException as e:
            print(f"Error scraping page {page_url}: {e}")
            break
        except Exception as e:
            print(f"An unexpected error occurred during scraping: {e}")
            break
    return scraped_data

def get_property_data_from_official_api(api_name='Zillow', location='Anytown', property_type='house'):
    """
    Simulates fetching property data from official Zillow or Realtor.com APIs.

    Args:
        api_name (str): 'Zillow' or 'Realtor'.
        location (str): Search location.
        property_type (str): Type of property (e.g., 'house', 'condo').

    Returns:
        list: A list of dictionaries, each representing a property from the API.
    """
    api_key = ZILLOW_API_KEY if api_name == 'Zillow' else REALTOR_API_KEY
    api_endpoint = ZILLOW_PROPERTY_API_ENDPOINT if api_name == 'Zillow' else REALTOR_LISTINGS_API_ENDPOINT

    print(f"--- Attempting to fetch data from {api_name} API for {location} ---")
    if api_key == "YOUR_ZILLOW_API_KEY" or api_key == "YOUR_REALTOR_API_KEY":
        print(f"Warning: Using placeholder API key for {api_name}. Real API calls will fail without a genuine key.")

    try:
        # In a real scenario, you'd construct the actual API request based on their docs:
        # headers = {"Accept": "application/json"}
        # params = {"location": location, "property_type": property_type, "api_key": api_key}
        # response = requests.get(api_endpoint, headers=headers, params=params)
        # response.raise_for_status()
        # api_raw_data = response.json()

        # --- Simulated API Response for Demonstration ---
        if api_name == 'Zillow':
            api_raw_data = {
                "properties": [
                    {"zpid": "P001", "address": {"street": "123 Main St", "city": "Anytown"}, "bedrooms": 3, "bathrooms": 2, "squareFootage": 1500, "zestimate": 350500, "lastSoldDate": "2020-05-10"},
                    {"zpid": "P002", "address": {"street": "456 Oak Ave", "city": "Anytown"}, "bedrooms": 4, "bathrooms": 3, "squareFootage": 2200, "zestimate": 505000, "lastSoldDate": "2021-01-20"},
                    {"zpid": "P003", "address": {"street": "789 Pine Ln", "city": "Anytown"}, "bedrooms": 2, "bathrooms": 1, "squareFootage": 900, "zestimate": 200100, "lastSoldDate": "2019-11-15"}
                ],
                "status": "success"
            }
        elif api_name == 'Realtor':
            api_raw_data = {
                "listings": [
                    {"listing_id": "L001", "location": {"address": "123 Main St, Anytown"}, "beds": 3, "baths": 2, "sq_ft": 1500, "list_price": 350000, "sale_date": "2020-05-10"},
                    {"listing_id": "L002", "location": {"address": "456 Oak Ave, Anytown"}, "beds": 4, "baths": 3, "sq_ft": 2200, "list_price": 500000, "sale_date": "2021-01-20"},
                    {"listing_id": "L003", "location": {"address": "789 Pine Ln, Anytown"}, "beds": 2, "baths": 1, "sq_ft": 900, "list_price": 200000, "sale_date": "2019-11-15"}
                ],
                "status": "success"
            }
        else:
            api_raw_data = {"status": "error", "message": "Invalid API name"}
        # --- End Simulated API Response ---

        if api_raw_data.get("status") == "success":
            if api_name == 'Zillow':
                # Normalize Zillow data
                normalized_data = []
                for prop in api_raw_data.get("properties", []):
                    normalized_data.append({
                        'api_source': 'Zillow',
                        'property_api_id': prop.get('zpid'),
                        'address': f"{prop.get('address', {}).get('street')}, {prop.get('address', {}).get('city')}",
                        'beds': prop.get('bedrooms'),
                        'baths': prop.get('bathrooms'),
                        'sqft': prop.get('squareFootage'),
                        'price': prop.get('zestimate'),
                        'last_sold_date': prop.get('lastSoldDate')
                    })
                print(f"Successfully fetched and normalized data from {api_name} API.")
                return normalized_data
            elif api_name == 'Realtor':
                # Normalize Realtor.com data
                normalized_data = []
                for listing in api_raw_data.get("listings", []):
                    normalized_data.append({
                        'api_source': 'Realtor',
                        'property_api_id': listing.get('listing_id'),
                        'address': listing.get('location', {}).get('address'),
                        'beds': listing.get('beds'),
                        'baths': listing.get('baths'),
                        'sqft': listing.get('sq_ft'),
                        'price': listing.get('list_price'),
                        'last_sold_date': listing.get('sale_date')
                    })
                print(f"Successfully fetched and normalized data from {api_name} API.")
                return normalized_data
        else:
            print(f"{api_name} API call failed: {api_raw_data.get('message', 'Unknown error')}")
            return []
    except requests.exceptions.RequestException as e:
        print(f"Error during {api_name} API call: {e}")
        return []
    except Exception as e:
        print(f"An unexpected error occurred processing {api_name} API data: {e}")
        return []

def aggregate_real_estate_data(crm_df, scraped_data_list, api_data_list):
    """
    Aggregates data from different sources into a unified DataFrame.
    This involves identifying common keys (e.g., property ID, address)
    and merging/joining the DataFrames.

    Args:
        crm_df (pd.DataFrame): DataFrame from CRM.
        scraped_data_list (list): List of dictionaries from web scraping.
        api_data_list (list): List of dictionaries from API.

    Returns:
        pd.DataFrame: A unified DataFrame containing combined real estate data.
    """
    print("\n--- Aggregating Data from Multiple Sources ---")

    # Convert scraped and API data to DataFrames
    df_scraped = pd.DataFrame(scraped_data_list)
    df_api = pd.DataFrame(api_data_list)

    # Start with CRM data as the base for tenant-centric view
    unified_df = crm_df.copy()
    print(f"Starting with CRM data. Shape: {unified_df.shape}")

    # Merge with API data (using property_id from CRM and property_api_id from API)
    if not df_api.empty:
        # Pre-process API data to ensure 'property_id' is consistent for merging
        # This is a simplification; real matching might need fuzzy string matching on address
        df_api['property_id'] = df_api['property_api_id'] # Assume direct mapping for demo

        # Select relevant API columns and drop duplicates based on property_id
        api_cols_to_merge = ['property_id', 'beds', 'baths', 'sqft', 'price', 'last_sold_date']
        df_api_filtered = df_api[api_cols_to_merge].drop_duplicates(subset=['property_id'])

        # Merge API data into unified_df
        unified_df = pd.merge(unified_df, df_api_filtered, on='property_id', how='left', suffixes=('_crm', '_api'))
        print(f"Merged with API data. New shape: {unified_df.shape}")
    else:
        print("No API data to merge.")

    # Integrate scraped data (often harder to merge directly due to lack of common IDs)
    # For demonstration, we'll add scraped data as new columns if addresses match conceptually.
    # In a real scenario, this would involve robust address matching or Geo-coding.
    if not df_scraped.empty:
        print(f"Scraped data available. Shape: {df_scraped.shape}. Manual or fuzzy matching may be needed.")
        # Example: If we could match by address, we'd do another merge here
        # unified_df = pd.merge(unified_df, df_scraped, left_on='address_from_crm', right_on='scraped_address', how='left')
        # For this demo, we'll just add a placeholder column if scraped data exists
        unified_df['has_scraped_listing'] = unified_df['property_id'].apply(
            lambda pid: 1 if any(f"P{i}" in s['scraped_address'] for i, s in enumerate(scraped_data_list)) else 0
        )
        print("Added 'has_scraped_listing' indicator (conceptual).")
    else:
        print("No scraped data to integrate.")

    # Post-aggregation cleaning/harmonization
    # Resolve conflicting columns, e.g., 'beds_crm' vs 'beds_api'
    for col in ['beds', 'baths', 'sqft', 'price']:
        if f'{col}_api' in unified_df.columns and f'{col}_crm' in unified_df.columns:
            # Prioritize API data if available, otherwise use CRM data
            unified_df[col] = unified_df[f'{col}_api'].fillna(unified_df[f'{col}_crm'])
            unified_df.drop(columns=[f'{col}_crm', f'{col}_api'], inplace=True)
        elif f'{col}_api' in unified_df.columns:
            unified_df.rename(columns={f'{col}_api': col}, inplace=True)
        elif f'{col}_crm' in unified_df.columns:
            unified_df.rename(columns={f'{col}_crm': col}, inplace=True)
        # Ensure numerical type
        if col in unified_df.columns:
            unified_df[col] = pd.to_numeric(unified_df[col], errors='coerce')
            unified_df[col].fillna(unified_df[col].median(), inplace=True) # Fill any NaNs after merge

    # Handle remaining date columns from API if they exist and are not already in CRM
    if 'last_sold_date' in unified_df.columns:
        unified_df['last_sold_date'] = pd.to_datetime(unified_df['last_sold_date'], errors='coerce')
        unified_df['days_since_last_sold'] = (pd.to_datetime('now') - unified_df['last_sold_date']).dt.days.fillna(0)

    # Drop temporary/raw columns from API/scraped data if not needed
    unified_df.drop(columns=['api_source', 'property_api_id'] if 'api_source' in unified_df.columns else [], inplace=True, errors='ignore')

    print(f"Final unified DataFrame shape: {unified_df.shape}")
    return unified_df

# --- Data Preprocessing and Analysis Functions (from previous module) ---

def perform_data_cleaning(df):
    """
    Cleans the DataFrame by handling missing values, converting data types,
    and creating derived features.

    Args:
        df (pd.DataFrame): The input DataFrame.

    Returns:
        pd.DataFrame: The cleaned DataFrame.
    """
    print("--- Performing Data Cleaning and Type Conversion ---")

    # Convert date columns to datetime objects
    date_cols = ['lease_start_date', 'lease_end_date', 'last_sold_date']
    for col in date_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')

    # Convert numerical columns to numeric, coercing errors to NaN
    numeric_cols = [
        'monthly_rent', 'payment_delays_last_6_months',
        'maintenance_requests_last_year', 'feedback_score_avg',
        'beds', 'baths', 'sqft', 'price'
    ]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # Handle missing values:
    # For numerical columns, fill with median (robust to outliers)
    for col in numeric_cols:
        if col in df.columns and df[col].isnull().any():
            median_val = df[col].median()
            df[col].fillna(median_val, inplace=True)
            print(f"Filled missing values in '{col}' with median: {median_val}")

    # For date columns, if missing, you might drop rows or impute with a sensible default
    # For this example, we'll drop rows if critical dates are missing
    df.dropna(subset=['lease_start_date', 'lease_end_date'], inplace=True)
    print("Dropped rows with missing critical date information.")

    # Create derived features (useful for both churn and maintenance)
    df['lease_duration_days'] = (df['lease_end_date'] - df['lease_start_date']).dt.days
    df['days_since_last_sold'] = (pd.to_datetime('now') - df['last_sold_date']).dt.days.fillna(0) # Assuming current date

    print("Data cleaning and type conversion complete.")
    return df

def handle_outliers(df, columns, strategy='cap'):
    """
    Handles outliers in specified numerical columns using the IQR method.

    Args:
        df (pd.DataFrame): The input DataFrame.
        columns (list): List of numerical columns to check for outliers.
        strategy (str): 'cap' to cap outliers, 'remove' to remove rows with outliers.

    Returns:
        pd.DataFrame: DataFrame with outliers handled.
    """
    print(f"\n--- Handling Outliers ({strategy} strategy) ---")
    df_cleaned = df.copy()
    outlier_detected = False

    for col in columns:
        if col in df_cleaned.columns and pd.api.types.is_numeric_dtype(df_cleaned[col]):
            Q1 = df_cleaned[col].quantile(0.25)
            Q3 = df_cleaned[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            num_outliers = ((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)).sum()
            if num_outliers > 0:
                outlier_detected = True
                print(f"  Outliers detected in '{col}': {num_outliers} values outside [{lower_bound:.2f}, {upper_bound:.2f}]")

                if strategy == 'cap':
                    df_cleaned[col] = np.where(df_cleaned[col] < lower_bound, lower_bound, df_cleaned[col])
                    df_cleaned[col] = np.where(df_cleaned[col] > upper_bound, upper_bound, df_cleaned[col])
                    print(f"  Outliers in '{col}' capped to bounds.")
                elif strategy == 'remove':
                    initial_rows = len(df_cleaned)
                    df_cleaned = df_cleaned[~((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound))]
                    print(f"  Removed {initial_rows - len(df_cleaned)} rows with outliers in '{col}'.")

    if not outlier_detected:
        print("No significant outliers detected in specified columns using IQR method.")

    return df_cleaned

def drop_redundant_values_and_columns(df):
    """
    Identifies and drops redundant columns (e.g., duplicates, highly correlated)
    or values.

    Args:
        df (pd.DataFrame): The input DataFrame.

    Returns:
        pd.DataFrame: DataFrame with redundant data removed.
    """
    print("\n--- Dropping Redundant Values/Columns ---")

    initial_cols = df.columns.tolist()

    # Drop columns with a single unique value (if they exist and are not meaningful)
    cols_to_drop_single_value = [col for col in df.columns if df[col].nunique() <= 1 and col not in ['lease_renewal', 'failure_imminent']]
    if cols_to_drop_single_value:
        df.drop(columns=cols_to_drop_single_value, inplace=True)
        print(f"Dropped columns with single unique value: {cols_to_drop_single_value}")

    # Drop columns that are exact duplicates of another column
    transpose_df = df.T
    duplicated_cols = transpose_df[transpose_df.duplicated()].index.tolist()
    if duplicated_cols:
        df.drop(columns=duplicated_cols, inplace=True)
        print(f"Dropped exact duplicate columns: {duplicated_cols}")

    # Identify and potentially drop highly correlated numerical columns
    numeric_df = df.select_dtypes(include=np.number)
    if not numeric_df.empty:
        corr_matrix = numeric_df.corr().abs()
        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop_high_corr = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)] # Threshold 0.95

        if to_drop_high_corr:
            print(f"Highly correlated columns (consider dropping one from each pair): {to_drop_high_corr}")
            # Example: df.drop(columns=to_drop_high_corr, inplace=True)
        else:
            print("No highly correlated numerical columns (above 0.95) found to drop automatically.")

    print("Redundancy check complete.")
    return df

def statistical_analysis_summary(df):
    """
    Provides a comprehensive statistical summary of the DataFrame,
    including various visualizations for better data understanding.

    Args:
        df (pd.DataFrame): The input DataFrame.
    """
    print("\n--- Statistical Analysis Summary ---")

    print("\nDataFrame Info:")
    df.info()

    print("\nDescriptive Statistics for Numerical Columns:")
    print(df.describe().T)

    print("\nValue Counts for Key Categorical/Discrete Columns:")
    categorical_cols = ['property_id', 'lease_renewal', 'beds', 'baths'] # Added beds, baths for bar plots
    for col in categorical_cols:
        if col in df.columns:
            print(f"\n--- {col} Distribution ---")
            print(df[col].value_counts(normalize=True) * 100) # Percentage distribution

    print("\nCorrelation Matrix (Numerical Features):")
    numerical_df = df.select_dtypes(include=np.number)
    if not numerical_df.empty:
        plt.figure(figsize=(10, 8))
        sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
        plt.title('Correlation Matrix of Numerical Features')
        plt.show()
    else:
        print("No numerical columns for correlation analysis.")

    # --- NEW VISUALIZATION FEATURES ---

    print("\n--- Visualizing Feature Distributions ---")
    numerical_features_to_plot = [
        'monthly_rent', 'payment_delays_last_6_months',
        'maintenance_requests_last_year', 'feedback_score_avg',
        'sqft', 'price', 'lease_duration_days', 'days_since_last_sold'
    ]

    # Histograms for numerical features
    num_plots = len(numerical_features_to_plot)
    if num_plots > 0:
        fig, axes = plt.subplots(nrows=int(np.ceil(num_plots/3)), ncols=3, figsize=(18, 5 * int(np.ceil(num_plots/3))))
        axes = axes.flatten() # Flatten for easy iteration

        for i, col in enumerate(numerical_features_to_plot):
            if col in df.columns:
                sns.histplot(df[col], kde=True, ax=axes[i])
                axes[i].set_title(f'Distribution of {col}')
                axes[i].set_xlabel(col)
                axes[i].set_ylabel('Frequency')
            else:
                axes[i].set_visible(False) # Hide unused subplots
        plt.tight_layout()
        plt.show()
    else:
        print("No numerical features available for distribution plots.")

    # Bar plots for key categorical/discrete features
    categorical_features_to_plot = ['lease_renewal', 'beds', 'baths']
    num_plots_cat = len(categorical_features_to_plot)
    if num_plots_cat > 0:
        fig_cat, axes_cat = plt.subplots(nrows=1, ncols=num_plots_cat, figsize=(6 * num_plots_cat, 6))
        if num_plots_cat == 1: # Handle single subplot case
            axes_cat = [axes_cat]

        for i, col in enumerate(categorical_features_to_plot):
            if col in df.columns:
                sns.countplot(x=col, data=df, ax=axes_cat[i], palette='viridis')
                axes_cat[i].set_title(f'Count of {col}')
                axes_cat[i].set_xlabel(col)
                axes_cat[i].set_ylabel('Count')
                if col == 'lease_renewal':
                    axes_cat[i].set_xticklabels(['Churn (0)', 'Renewal (1)'])
            else:
                axes_cat[i].set_visible(False)
        plt.tight_layout()
        plt.show()
    else:
        print("No categorical features available for count plots.")

    print("\n--- Visualizing Relationships Between Key Features ---")

    # Scatter plot: Square Footage vs. Price
    if 'sqft' in df.columns and 'price' in df.columns:
        plt.figure(figsize=(10, 6))
        sns.scatterplot(x='sqft', y='price', data=df, alpha=0.6)
        plt.title('Property Price vs. Square Footage')
        plt.xlabel('Square Footage')
        plt.ylabel('Price')
        plt.grid(True)
        plt.show()
    else:
        print("Cannot plot Price vs. Square Footage: 'sqft' or 'price' column missing.")

    # Scatter plot: Monthly Rent vs. Square Footage
    if 'monthly_rent' in df.columns and 'sqft' in df.columns:
        plt.figure(figsize=(10, 6))
        sns.scatterplot(x='sqft', y='monthly_rent', data=df, alpha=0.6)
        plt.title('Monthly Rent vs. Square Footage')
        plt.xlabel('Square Footage')
        plt.ylabel('Monthly Rent')
        plt.grid(True)
        plt.show()
    else:
        print("Cannot plot Monthly Rent vs. Square Footage: 'monthly_rent' or 'sqft' column missing.")

    # Box plot: Monthly Rent by Lease Renewal Status
    if 'monthly_rent' in df.columns and 'lease_renewal' in df.columns:
        plt.figure(figsize=(8, 6))
        sns.boxplot(x='lease_renewal', y='monthly_rent', data=df, palette='pastel')
        plt.title('Monthly Rent Distribution by Lease Renewal Status')
        plt.xlabel('Lease Renewal (0: Churn, 1: Renew)')
        plt.ylabel('Monthly Rent')
        plt.show()
    else:
        print("Cannot plot Monthly Rent by Lease Renewal: 'monthly_rent' or 'lease_renewal' column missing.")

    # --- END NEW VISUALIZATION FEATURES ---

def customer_behavior_summary(df):
    """
    Provides a summary of customer (tenant) behavior metrics.

    Args:
        df (pd.DataFrame): The input DataFrame.
    """
    print("\n--- Customer Behavior Summary ---")

    # Debugging: Print columns to verify
    print("Columns in DataFrame for customer_behavior_summary:", df.columns.tolist())

    if 'lease_renewal' in df.columns:
        churn_rate = (df['lease_renewal'] == 0).mean() * 100
        print(f"Overall Tenant Churn Rate: {churn_rate:.2f}%")

    # Check for existence of all columns before trying to access them
    required_cols_mean = ['payment_delays_last_6_months', 'maintenance_requests_last_year', 'feedback_score_avg']
    present_cols_mean = [col for col in required_cols_mean if col in df.columns]

    if present_cols_mean:
        print("\nAverage Payment Delays and Maintenance Requests:")
        print(df[present_cols_mean].mean())
    else:
        print("\nCould not calculate average payment delays and maintenance requests: Required columns missing.")


    print("\nCustomer Behavior by Lease Renewal Status:")
    if 'lease_renewal' in df.columns:
        group_cols = ['payment_delays_last_6_months', 'maintenance_requests_last_year', 'feedback_score_avg', 'monthly_rent']
        present_group_cols = [col for col in group_cols if col in df.columns]
        if present_group_cols:
            print(df.groupby('lease_renewal')[present_group_cols].mean())
        else:
            print("Could not group by lease renewal status: Required columns missing.")
    else:
        print("Cannot summarize by lease renewal status: 'lease_renewal' column missing.")

    print("\nTop Properties by Maintenance Requests:")
    if 'property_id' in df.columns and 'maintenance_requests_last_year' in df.columns:
        prop_maintenance = df.groupby('property_id')['maintenance_requests_last_year'].sum().sort_values(ascending=False)
        print(prop_maintenance.head())
    else:
        print("Cannot summarize top properties by maintenance requests: 'property_id' or 'maintenance_requests_last_year' column missing.")

    print("\nDistribution of Feedback Scores:")
    if 'feedback_score_avg' in df.columns:
        plt.figure(figsize=(8, 5))
        sns.histplot(df['feedback_score_avg'], bins=10, kde=True)
        plt.title('Distribution of Average Tenant Feedback Scores')
        plt.xlabel('Feedback Score')
        plt.ylabel('Number of Tenants')
        plt.show()
    else:
        print("Cannot plot feedback score distribution: 'feedback_score_avg' column missing.")

def prepare_for_modeling(df, target_column_classification='lease_renewal', target_column_regression='monthly_rent'):
    """
    Prepares the DataFrame for machine learning modeling.
    This includes encoding categorical variables, separating features from the target,
    and scaling numerical features.

    Args:
        df (pd.DataFrame): The input DataFrame.
        target_column_classification (str): The name of the classification target variable column.
        target_column_regression (str): The name of the regression target variable column.

    Returns:
        tuple: (X_classification, y_classification, X_regression, y_regression, preprocessed_df)
               X_classification: Features DataFrame for classification,
               y_classification: Target Series for classification,
               X_regression: Features DataFrame for regression,
               y_regression: Target Series for regression,
               preprocessed_df: DataFrame ready for visualization (with encoded features).
    """
    print("\n--- Preparing Data for Modeling and Visualization ---")

    df_model_ready = df.copy()

    # Drop date columns as they are not directly used in most ML models (derived features are used)
    date_cols_to_drop = ['lease_start_date', 'lease_end_date', 'last_sold_date']
    df_model_ready.drop(columns=[col for col in date_cols_to_drop if col in df_model_ready.columns], inplace=True)
    print(f"Dropped original date columns: {date_cols_to_drop}")

    # Encode categorical features (e.g., 'property_id' if treated as categorical)
    if 'property_id' in df_model_ready.columns:
        le = LabelEncoder()
        df_model_ready['property_id_encoded'] = le.fit_transform(df_model_ready['property_id'])
        print(f"Encoded 'property_id' to numerical: {le.classes_} -> {le.transform(le.classes_)}")
        df_model_ready.drop(columns=['property_id'], inplace=True) # Drop original string column

    # Drop 'tenant_id' as it's an identifier, not a feature
    if 'tenant_id' in df_model_ready.columns:
        df_model_ready.drop(columns=['tenant_id'], inplace=True)
        print("Dropped 'tenant_id' column.")

    # --- Separate features (X) and targets (y) for different tasks ---
    X_classification = None
    y_classification = None
    X_regression = None
    y_regression = None

    # For Classification (Tenant Churn)
    if target_column_classification in df_model_ready.columns:
        y_classification = df_model_ready[target_column_classification]
        X_classification = df_model_ready.drop(columns=[target_column_classification])
        # If regression target is also in X_classification, drop it
        if target_column_regression in X_classification.columns:
            X_classification = X_classification.drop(columns=[target_column_regression])
        print(f"Separated classification target '{target_column_classification}' from features.")
    else:
        print(f"Warning: Classification target column '{target_column_classification}' not found.")
        X_classification = df_model_ready.drop(columns=[target_column_regression]) if target_column_regression in df_model_ready.columns else df_model_ready

    # For Regression (Monthly Rent Prediction)
    if target_column_regression in df_model_ready.columns:
        y_regression = df_model_ready[target_column_regression]
        X_regression = df_model_ready.drop(columns=[target_column_regression])
        # If classification target is also in X_regression, drop it
        if target_column_classification in X_regression.columns:
            X_regression = X_regression.drop(columns=[target_column_classification])
        print(f"Separated regression target '{target_column_regression}' from features.")
    else:
        print(f"Warning: Regression target column '{target_column_regression}' not found.")
        X_regression = df_model_ready.drop(columns=[target_column_classification]) if target_column_classification in df_model_ready.columns else df_model_ready

    # Identify numerical columns for scaling
    numerical_cols_classification = X_classification.select_dtypes(include=np.number).columns.tolist() if X_classification is not None else []
    numerical_cols_regression = X_regression.select_dtypes(include=np.number).columns.tolist() if X_regression is not None else []

    # Scale numerical features for classification
    scaler_classification = StandardScaler()
    if X_classification is not None and not X_classification.empty and numerical_cols_classification:
        X_classification_scaled = X_classification.copy()
        X_classification_scaled[numerical_cols_classification] = scaler_classification.fit_transform(X_classification[numerical_cols_classification])
        X_classification = X_classification_scaled
        print("Numerical features for classification scaled.")
    else:
        print("No numerical features to scale for classification or X_classification is empty.")

    # Scale numerical features for regression
    scaler_regression = StandardScaler()
    if X_regression is not None and not X_regression.empty and numerical_cols_regression:
        X_regression_scaled = X_regression.copy()
        X_regression_scaled[numerical_cols_regression] = scaler_regression.fit_transform(X_regression[numerical_cols_regression])
        X_regression = X_regression_scaled
        print("Numerical features for regression scaled.")
    else:
        print("No numerical features to scale for regression or X_regression is empty.")

    print(f"Features (X_classification) shape: {X_classification.shape if X_classification is not None else 'N/A'}")
    print(f"Target (y_classification) shape: {y_classification.shape if y_classification is not None else 'N/A'}")
    print(f"Features (X_regression) shape: {X_regression.shape if X_regression is not None else 'N/A'}")
    print(f"Target (y_regression) shape: {y_regression.shape if y_regression is not None else 'N/A'}")
    print("Data ready for modeling.")
    return X_classification, y_classification, X_regression, y_regression, df_model_ready # Return df_model_ready for visualization with encoded features

# --- New Machine Learning Model Functions ---

def train_evaluate_classification_model(X, y, model, model_name):
    """
    Trains and evaluates a classification model.

    Args:
        X (pd.DataFrame): Features DataFrame.
        y (pd.Series): Target Series.
        model: The scikit-learn classification model object.
        model_name (str): Name of the model for printing.
    """
    print(f"\n--- Training and Evaluating {model_name} for Tenant Churn Prediction ---")
    if X is None or y is None or X.empty:
        print(f"Skipping {model_name}: X or y is None or empty.")
        return

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy ({model_name}): {accuracy:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Churn', 'Renewal']))

    # Plot Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(7, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Predicted Churn', 'Predicted Renewal'],
                yticklabels=['Actual Churn', 'Actual Renewal'])
    plt.title(f'Confusion Matrix for {model_name}')
    plt.ylabel('Actual Label')
    plt.xlabel('Predicted Label')
    plt.show()

def train_evaluate_regression_model(X, y, model, model_name, target_name="Target"):
    """
    Trains and evaluates a regression model.

    Args:
        X (pd.DataFrame): Features DataFrame.
        y (pd.Series): Target Series.
        model: The scikit-learn regression model object.
        model_name (str): Name of the model for printing.
        target_name (str): Name of the target variable for plots.
    """
    print(f"\n--- Training and Evaluating {model_name} for {target_name} Prediction ---")
    if X is None or y is None or X.empty:
        print(f"Skipping {model_name}: X or y is None or empty.")
        return

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"Mean Squared Error ({model_name}): {mse:.4f}")
    print(f"R-squared ({model_name}): {r2:.4f}")

    # Plot Actual vs. Predicted values
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)
    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Perfect Prediction')
    plt.title(f'Actual vs. Predicted {target_name} ({model_name})')
    plt.xlabel(f'Actual {target_name}')
    plt.ylabel(f'Predicted {target_name}')
    plt.grid(True)
    plt.legend()
    plt.show()

def train_evaluate_neural_network_classification(X, y, model_name="Neural Network"):
    """
    Trains and evaluates a simple Neural Network for classification.

    Args:
        X (pd.DataFrame): Features DataFrame (should be scaled).
        y (pd.Series): Target Series.
        model_name (str): Name of the model for printing.
    """
    if tf is None:
        print(f"Skipping {model_name}: TensorFlow not installed.")
        return
    if X is None or y is None or X.empty:
        print(f"Skipping {model_name}: X or y is None or empty.")
        return

    print(f"\n--- Training and Evaluating {model_name} for Tenant Churn Prediction ---")

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

    # Define the Neural Network model
    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
        layers.Dropout(0.3), # Dropout for regularization
        layers.Dense(32, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(1, activation='sigmoid') # Sigmoid for binary classification
    ])

    # Compile the model
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    # Train the model
    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0) # verbose=0 to reduce output

    # Evaluate the model on the test set
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"Test Accuracy ({model_name}): {accuracy:.4f}")

    # Make predictions (probabilities)
    y_pred_proba = model.predict(X_test, verbose=0).flatten()
    y_pred = (y_pred_proba > 0.5).astype(int)

    # Print classification report and confusion matrix
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Churn', 'Renewal']))

    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(7, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',
                xticklabels=['Predicted Churn', 'Predicted Renewal'],
                yticklabels=['Actual Churn', 'Actual Renewal'])
    plt.title(f'Confusion Matrix for {model_name}')
    plt.ylabel('Actual Label')
    plt.xlabel('Predicted Label')
    plt.show()

    # Plot training history
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'{model_name} Training Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'{model_name} Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.tight_layout()
    plt.show()

def perform_hyperparameter_optimization(X_train, y_train, model_type, model_name):
    """
    Performs hyperparameter optimization using GridSearchCV for a given model type.

    Args:
        X_train (pd.DataFrame): Training features.
        y_train (pd.Series): Training target.
        model_type (str): 'classification' or 'regression'.
        model_name (str): Name of the model (e.g., 'RandomForestClassifier', 'RandomForestRegressor').

    Returns:
        best_estimator: The best model found by GridSearchCV.
    """
    print(f"\n--- Performing Hyperparameter Optimization for {model_name} ---")
    if X_train is None or y_train is None or X_train.empty:
        print(f"Skipping optimization for {model_name}: Training data is None or empty.")
        return None

    if model_type == 'classification':
        model = RandomForestClassifier(random_state=42)
        param_grid = {
            'n_estimators': [50, 100, 150],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2]
        }
        scoring = 'accuracy'
    elif model_type == 'regression':
        model = RandomForestRegressor(random_state=42)
        param_grid = {
            'n_estimators': [50, 100, 150],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2]
        }
        scoring = 'neg_mean_squared_error' # For regression, GridSearchCV optimizes for higher scores
    else:
        print(f"Invalid model_type: {model_type}. Must be 'classification' or 'regression'.")
        return None

    grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                               scoring=scoring, cv=3, n_jobs=-1, verbose=1)

    grid_search.fit(X_train, y_train)

    print(f"\nBest parameters for {model_name}: {grid_search.best_params_}")
    print(f"Best cross-validation score for {model_name}: {grid_search.best_score_:.4f}")

    return grid_search.best_estimator_

    # --- Main Execution Flow for Combined Pipeline ---
if __name__ == "__main__":
    print("--- Starting Combined Real Estate Data Pipeline ---")

    # --- Data Ingestion Phase ---
    crm_data = load_crm_data_live()
    print("\n--- CRM Data Loaded (Conceptual Live) ---")
    print(crm_data.head())

    scraped_data = scrape_public_listings(search_url='http://quotes.toscrape.com/', num_pages=1) # Using safe URL
    print("\n--- Scraped Data Loaded (Conceptual) ---")
    if scraped_data:
        print(pd.DataFrame(scraped_data).head())
    else:
        print("No scraped data to display.")

    zillow_api_data = get_property_data_from_official_api(api_name='Zillow', location='Anytown')
    realtor_api_data = get_property_data_from_official_api(api_name='Realtor', location='Anytown')
    api_data_combined = zillow_api_data + realtor_api_data

    print("\n--- Combined API Data Loaded (Conceptual Official) ---")
    if api_data_combined:
        print(pd.DataFrame(api_data_combined).head())
    else:
        print("No API data to display.")

    # Aggregate all data sources
    if not crm_data.empty:
        unified_df = aggregate_real_estate_data(crm_data, scraped_data, api_data_combined)
        print("\n--- Final Unified Data Sample after Ingestion ---")
        print(unified_df.head())
        print(f"\nFinal unified data columns: {unified_df.columns.tolist()}")
    else:
        print("\nCRM data was not loaded. Cannot proceed with aggregation and preprocessing.")
        unified_df = pd.DataFrame() # Ensure unified_df is defined as empty if CRM fails

    # --- Data Preprocessing and Analysis Phase ---
    if not unified_df.empty:
        cleaned_df = perform_data_cleaning(unified_df.copy())
        print("\nCleaned DataFrame Head:")
        print(cleaned_df.head())
        cleaned_df.info()

        outlier_cols = ['monthly_rent', 'payment_delays_last_6_months', 'maintenance_requests_last_year', 'sqft', 'price']
        df_no_outliers = handle_outliers(cleaned_df.copy(), outlier_cols, strategy='cap')
        print("\nDataFrame after Outlier Handling (Capped) Head:")
        print(df_no_outliers.head())

        df_processed = drop_redundant_values_and_columns(df_no_outliers.copy())
        print("\nDataFrame after Redundancy Removal Head:")
        print(df_processed.head())
        print(f"Columns after redundancy removal: {df_processed.columns.tolist()}")

        statistical_analysis_summary(df_processed.copy())

        customer_behavior_summary(df_processed.copy())

        # Prepare data for both classification (churn) and regression (rent)
        X_classification_final, y_classification_final, X_regression_final, y_regression_final, df_for_viz = \
            prepare_for_modeling(df_processed.copy(), target_column_classification='lease_renewal', target_column_regression='monthly_rent')

        print("\n--- Final Data for Modeling (Features X for Classification) Head ---")
        if X_classification_final is not None:
            print(X_classification_final.head())
        else:
            print("X_classification_final is None.")

        print("\n--- Final Data for Modeling (Target y for Classification) Head ---")
        if y_classification_final is not None:
            print(y_classification_final.head())
        else:
            print("y_classification_final is None, cannot display head.")

        print("\n--- Final Data for Modeling (Features X for Regression) Head ---")
        if X_regression_final is not None:
            print(X_regression_final.head())
        else:
            print("X_regression_final is None.")

        print("\n--- Final Data for Modeling (Target y for Regression) Head ---")
        if y_regression_final is not None:
            print(y_regression_final.head())
        else:
            print("y_regression_final is None, cannot display head.")

        print("\n--- Data for Visualization (df_for_viz) Head (with encoded features) ---")
        print(df_for_viz.head())

        # --- Machine Learning Model Training and Evaluation Phase ---

        # 1. Tenant Churn Prediction (Classification)
        if y_classification_final is not None and not y_classification_final.empty:
            # Logistic Regression
            log_reg_model = LogisticRegression(random_state=42, max_iter=1000)
            train_evaluate_classification_model(X_classification_final, y_classification_final, log_reg_model, "Logistic Regression")

            # Random Forest Classifier
            rf_classifier_model = RandomForestClassifier(n_estimators=100, random_state=42)
            train_evaluate_classification_model(X_classification_final, y_classification_final, rf_classifier_model, "Random Forest Classifier")

            # Neural Network for Classification
            if tf is not None:
                train_evaluate_neural_network_classification(X_classification_final, y_classification_final, "Neural Network (Tenant Churn)")
            else:
                print("\nSkipping Neural Network for Classification: TensorFlow not installed.")
        else:
            print("\nSkipping Tenant Churn Prediction models: Classification target data is not available.")


        # 2. Monthly Rent Prediction (Regression)
        if y_regression_final is not None and not y_regression_final.empty:
            # Random Forest Regressor
            rf_regressor_model = RandomForestRegressor(n_estimators=100, random_state=42)
            train_evaluate_regression_model(X_regression_final, y_regression_final, rf_regressor_model, "Random Forest Regressor", target_name="Monthly Rent")
        else:
            print("\nSkipping Monthly Rent Prediction model: Regression target data is not available.")

    else:
        print("\nUnified DataFrame is empty. Skipping preprocessing, analysis, and ML modeling.")